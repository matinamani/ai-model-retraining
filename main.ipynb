{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "    <h1 align=center>Retraining of All Training Data</h1>\n",
    "    <h3 align=center>Computational Intelligence</h3>\n",
    "</header>\n",
    "<main>\n",
    "    <font size=12>Members:</font>\n",
    "    <ol>\n",
    "        <li>Matin Amani</li>\n",
    "        <li>Motahare Hoseyni</li>\n",
    "        <li>Fateme Safayi</li>\n",
    "        <li>Shaghayegh Shirvani</li>\n",
    "    </ol>\n",
    "</main>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<h3>Why Retrain a Trained Model?</h3>\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "Retraining an AI model in machine learning is a crucial process that allows us to continually improve and adapt the model's performance over time. As new data becomes available or the problem domain changes, retraining becomes essential to ensure that the AI model remains accurate, relevant, and up to date. By exposing the model to additional data, it can learn from new patterns, trends, and insights that were not present during its initial training phase. Retraining helps the model to <font color=\"#09c\" size=6><u>refine its predictions</u></font>, <font color=\"#09c\" size=6><u>enhance its ability to generalize</u></font>, <font color=\"#09c\" size=6><u> adapt to evolving conditions</u></font> and ultimately leading to more robust and reliable results. Additionally, retraining also enables the model to address any biases or errors that may have been present in the original training data, promoting fairness and inclusivity in AI applications. Thus, retraining an AI model is a continuous iterative process that empowers it to stay competent, adaptive, and accountable in an ever-changing world.\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "<hr />\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "Now, let's dive into the code and see how retraining effects our models' predictions.\n",
    "</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "At the very top, we import our models.<br />\n",
    "Initially, we don't discuss how all these models are created. we just feed them some data & their configurations and wait for the results to come out.<br />\n",
    "These results will be plotted on graph and saved in the project directory.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import Mnist_Model\n",
    "from imdb import IMDB_Model\n",
    "from reuters import Reuters_Model\n",
    "from boston import Boston_Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:larger;\">\n",
    "After importing the models, we set some configurations for each models' hidden layers and a general test case.<br />\n",
    "These configurations will tell our model:\n",
    "<ul>\n",
    "    <li>how many hidden layer it will have</li>\n",
    "    <li>how each layer should behave</li>\n",
    "    and\n",
    "    <li>what learning rate and batch size should the network consider</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_layers = [\n",
    "    {\"neurons\": 512, \"activation\": \"relu\"},\n",
    "    {\"neurons\": 10, \"activation\": \"softmax\"},\n",
    "]\n",
    "\n",
    "imdb_layers = [\n",
    "    {\"neurons\": 16, \"activation\": \"relu\"},\n",
    "    {\"neurons\": 16, \"activation\": \"relu\"},\n",
    "    {\"neurons\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "reuters_layers = [\n",
    "    {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "    {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "    {\"neurons\": 46, \"activation\": \"softmax\"},\n",
    "]\n",
    "\n",
    "boston_layers = [\n",
    "    [\n",
    "        {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 32, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 1, \"activation\": None},\n",
    "    ],\n",
    "    [\n",
    "        {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 1, \"activation\": None},\n",
    "    ],\n",
    "    [\n",
    "        {\"neurons\": 64, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 128, \"activation\": \"relu\"},\n",
    "        {\"neurons\": 1, \"activation\": None},\n",
    "    ],\n",
    "]\n",
    "\n",
    "test_cases = [\n",
    "    {\"batch_size\": 128, \"learning_rate\": 0.001},\n",
    "    {\"batch_size\": 256, \"learning_rate\": 0.001},\n",
    "    {\"batch_size\": 512, \"learning_rate\": 0.001},\n",
    "    {\"batch_size\": 1024, \"learning_rate\": 0.001},\n",
    "    {\"batch_size\": 128, \"learning_rate\": 0.0001},\n",
    "    {\"batch_size\": 256, \"learning_rate\": 0.0001},\n",
    "    {\"batch_size\": 512, \"learning_rate\": 0.0001},\n",
    "    {\"batch_size\": 1024, \"learning_rate\": 0.0001},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 >\n",
    "First off, lets see <font color=\"#09c\">MNIST Dataset</font>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in test_cases:\n",
    "    model = Mnist_Model(\n",
    "        20, test_case[\"batch_size\"], test_case[\"learning_rate\"], mnist_layers\n",
    "    )\n",
    "    model.run()\n",
    "    model.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:larger;\">\n",
    "After running the cell above, these graphs will be saved in our projects' directory.<br /> Let's see what we have.\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:128_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:256_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:512_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:1024_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:128_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:256_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:512_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./mnist-fig;bs:1024_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "After analyzing all the graphs for MNIST dataset, we've come to the conclusion that:<br />\n",
    "<code style=\"font-size:large;\">{\"epochs\": 6, \"batch_size\": 128, \"learning_rate\": 0.001}</code>\n",
    "<br />\n",
    "is the best configuration for <font color=\"#09c\"><u>this network architecture</u></font>.\n",
    "</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "After MNIST, we run the same operation on <font color=\"#09c\">IMDB Dataset</font>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in test_cases:\n",
    "    model = IMDB_Model(\n",
    "        20,\n",
    "        test_case[\"batch_size\"],\n",
    "        test_case[\"learning_rate\"],\n",
    "        10000,\n",
    "        imdb_layers,\n",
    "    )\n",
    "    model.run()\n",
    "    model.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:larger;\">\n",
    "The graphs generated by the different models of IMDB are as below.<br /> Let's Analyze!\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:128_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:256_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:512_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:1024_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:128_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:256_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:512_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./imdb-fig;bs:1024_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "After analyzing all the graphs for IMDB dataset, we've come to the conclusion that:<br />\n",
    "<code style=\"font-size:large;\">{\"epochs\": 3, \"batch_size\": 128, \"learning_rate\": 0.001}</code>\n",
    "<br />\n",
    "is the best configuration for <font color=\"#09c\"><u>this network architecture</u></font>.\n",
    "</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "Let's repeat the same process for <font color=\"#09c\">Reuters Dataset</font>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in test_cases:\n",
    "    model = Reuters_Model(\n",
    "        20,\n",
    "        test_case[\"batch_size\"],\n",
    "        test_case[\"learning_rate\"],\n",
    "        10000,\n",
    "        reuters_layers,\n",
    "    )\n",
    "    model.run()\n",
    "    model.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:larger;\">\n",
    "The graphs generated by the different models of Reuters are as below.<br /> Let's Analyze!\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:128_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:256_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:512_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:1024_lr:0.001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 128 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:128_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 256 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:256_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 512 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:512_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>batch_size = 1024 ; learning_rate = 0.0001</code>\n",
    "        <br />\n",
    "        <img src=\"./reuters-fig;bs:1024_lr:0.0001.png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "After analyzing all the graphs for Reuters dataset, we've come to the conclusion that:<br />\n",
    "<code style=\"font-size:large;\">{\"epochs\": 5,\"batch_size\": 128, \"learning_rate\": 0.001}</code>\n",
    "<br />\n",
    "is the best configuration for <font color=\"#09c\"><u>this network architecture</u></font>.\n",
    "</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "Finally, let's train the <font color=\"#09c\">Boston Housing Dataset</font>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in boston_layers:\n",
    "    model = Boston_Model(500, 0.001, test_case)\n",
    "    model.run(4)\n",
    "    model.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:larger;\">\n",
    "Boston model graphs are a bit different!<br />\n",
    "in these graphs, the Y-Axis is <u>Mean Absolute Error</u>.<br />\n",
    "</p>\n",
    "\n",
    "<font size=18 align=center>\n",
    "\n",
    "$\\text{{MAE}} = \\frac{\\sum_{i=1}^{n} \\left| y_i - x_i \\right|}{n}$\n",
    "\n",
    "Which:<br />\n",
    "$y_i$ = predicted_value<br />\n",
    "$x_i$ = true_value\n",
    "\n",
    "</font>\n",
    "\n",
    "<p>\n",
    "Let's see which configuration gives us the minimum <code>MAE</code>.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <code>hidden_layers: [64, 32, 1]</code><br />\n",
    "        <img src=\"./boston-fig;layers:[64,32,1].png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>hidden_layers: [64, 64, 1]</code><br />\n",
    "        <img src=\"./boston-fig;layers:[64,64,1].png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "    <li>\n",
    "        <code>hidden_layers: [64, 128, 1]</code><br />\n",
    "        <img src=\"./boston-fig;layers:[64,128,1].png\"/>\n",
    "    </li>\n",
    "    <br />\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p style=\"font-size:larger;\">\n",
    "After analyzing all the graphs for Boston dataset, we've come to the conclusion that:<br />\n",
    "<code style=\"font-size:large;\">{\"neurons\": [64, 64, 1]}</code>\n",
    "<br />\n",
    "is the best network configuration for <font color=\"#09c\">Boston Housing Dataset</font>.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
